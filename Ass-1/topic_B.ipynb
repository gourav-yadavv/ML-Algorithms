{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic B: Estimating Parametric Models using MLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.1 Method Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of covariance matrix calculation and discriminant for the all the three models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Matrix : \n",
    "> The covariance matrix is a key parameter in the multivariate normal distribution, which describes the relationship between the different features (or variables) in the data. It is a square matrix that contains the variances of each variable along the diagonal, and the covariances between pairs of variables off the diagonal. In other words, it captures the extent to which two variables vary together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminant Function :\n",
    "\n",
    "> In a classification problem with multiple classes, we need to define a decision boundary that separates the different classes. Discriminant functions are a common way to define these decision boundaries. A discriminant function takes in a set of features and outputs a score for each class, based on how well the features match the distribution of that class. The class with the highest score is then chosen as the predicted class label.\n",
    "\n",
    "> In a parametric model based on the multivariate normal distribution, the discriminant functions are typically quadratic functions of the input features. These functions are derived from Bayes' theorem and the assumption of equal class priors, and can be expressed in terms of the means and covariance matrices of the class-conditional PDFs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](5.17.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](5.22.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](5.24.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class IndependentCovar:\n",
    "    # below function is to get covariance matrices \n",
    "    def getCovar(self, C1: pd.DataFrame, C2: pd.DataFrame,\n",
    "                 P_C1: float, P_C2: float,\n",
    "                 m1: np.array, m2: np.array):\n",
    "        # below is the calculation for covariance matrices for class C1 & C2\n",
    "        matrix1 = np.vstack([i-m1 for i in C1])\n",
    "        matrix2 = np.vstack([j-m2 for j in C2])\n",
    "        S1 = np.matmul(matrix1.T, matrix1)/C1.shape[0]\n",
    "        S2 = np.matmul(matrix2.T, matrix2)/C2.shape[0]\n",
    "        return S1, S2\n",
    "    # below is to function to classify a new datapoint and here the PC1 & PC2 are prior probabilities , m1 & m2 are mean of both the classes respectively and S1 & S2 are covariance matrices which we calculated in above function \n",
    "    def classify(self, x: pd.Series, P_C1: float, P_C2: float,\n",
    "                 m1: np.array, m2: np.array,\n",
    "                 S1: np.array, S2: np.array):\n",
    "        # this is the discriminant function eqn implementation of equation taken from 5.17 of Alpaydin & we calculated discriminant functions for both the classes \n",
    "        g1 = -0.5*(np.log(np.linalg.det(S1))\n",
    "                 + (x - m1).T.dot(np.linalg.inv(S1)).dot(x - m1))\\\n",
    "             + np.log(P_C1)\n",
    "        g2 = -0.5*(np.log(np.linalg.det(S2))\n",
    "                 + (x - m2).T.dot(np.linalg.inv(S2)).dot(x - m2))\\\n",
    "             + np.log(P_C2)\n",
    "        \n",
    "        # If discriminant functions are equal, classify randomly otherwise, classify based on which discriminant function is larger\n",
    "        if g1 == g2:\n",
    "            return random.choice([1, 2])\n",
    "        elif g1 > g2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "            \n",
    "class EqualCovar:\n",
    "    def getCovar(self, C1: pd.DataFrame, C2: pd.DataFrame,\n",
    "                 P_C1: float, P_C2: float,\n",
    "                 m1: np.array, m2:np.array):\n",
    "        matrix1 = np.vstack([i-m1 for i in C1])\n",
    "        matrix2 = np.vstack([j-m2 for j in C2])\n",
    "        S1 = np.matmul(matrix1.T, matrix1)/C1.shape[0]\n",
    "        S2 = np.matmul(matrix2.T, matrix2)/C2.shape[0]\n",
    "        # Compute the pooled covariance matrix\n",
    "        # The pooled covariance matrix is a weighted average of the covariance matrices of two or more groups or classes, where the weight of each group is proportional to the number of samples in that group. It is commonly used in statistical analysis and machine learning, particularly in classification problems where there are multiple groups or classes.\n",
    "        S = P_C1*S1 + P_C2*S2\n",
    "        return S, S\n",
    "    \n",
    "    def classify(self, x: pd.Series, P_C1: float, P_C2: float,\n",
    "                 m1: np.array, m2: np.array,\n",
    "                 S1: np.array, S2: np.array):\n",
    "        # this is the discriminant function eqn implementation of equation taken from 5.22 of Alpaydin & we calculated discriminant functions for both the classes \n",
    "        g1 = -0.5*(x - m1).T.dot(np.linalg.inv(S1)).dot(x - m1) + np.log(P_C1)\n",
    "        g2 = -0.5*(x - m2).T.dot(np.linalg.inv(S2)).dot(x - m2) + np.log(P_C2)\n",
    "        if g1 == g2: return random.choice([1, 2])\n",
    "        elif g1 > g2: return 1\n",
    "        else: return 2\n",
    "\n",
    "class DiagonalCovar:\n",
    "    def getCovar(self, C1: pd.DataFrame, C2: pd.DataFrame,\n",
    "                 P_C1: float, P_C2: float,\n",
    "                 m1: np.array, m2:np.array):\n",
    "        matrix1 = np.vstack([i-m1 for i in C1])\n",
    "        matrix2 = np.vstack([j-m2 for j in C2])\n",
    "        S1 = np.matmul(matrix1.T, matrix1)/C1.shape[0]\n",
    "        S2 = np.matmul(matrix2.T, matrix2)/C2.shape[0]\n",
    "        return np.diag(np.diag(S1)), np.diag(np.diag(S2))\n",
    "        \n",
    "    def classify(self, x: pd.Series, P_C1: float, P_C2: float,\n",
    "                     m1: np.array, m2: np.array,\n",
    "                     S1: np.array, S2: np.array):\n",
    "        # this is the discriminant function eqn implementation of equation taken from 5.22 of Alpaydin & we calculated discriminant functions for both the classes \n",
    "        g1 = -0.5*(np.log(np.linalg.det(S1))\n",
    "                 + (x - m1).T.dot(np.linalg.inv(S1)).dot(x - m1))\\\n",
    "             + np.log(P_C1)\n",
    "        g2 = -0.5*(np.log(np.linalg.det(S2))\n",
    "                 + (x - m2).T.dot(np.linalg.inv(S2)).dot(x - m2))\\\n",
    "             + np.log(P_C2)\n",
    "        if g1 == g2: return random.choice([1, 2])\n",
    "        elif g1 > g2: return 1\n",
    "        else: return 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Error function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testError(model, test: pd.DataFrame,\n",
    "              P_C1: float, P_C2: float,\n",
    "              m1: np.array, m2: np.array,\n",
    "              S1: np.array, S2: np.array):\n",
    "    actual_values = [i for i in test[8]]\n",
    "    predicted_values = [None for _ in actual_values]\n",
    "    test = test.drop(8, axis=1)\n",
    "    # Iterating through each row and predicting the class using classifier\n",
    "    for i in range(test.shape[0]):\n",
    "        predicted_values[i] = model.classify(test.iloc[i],P_C1, P_C2, m1, m2, S1, S2)\n",
    "    MSE = sum([abs(a - p) for a, p in zip(actual_values, predicted_values)])/test.shape[0]\n",
    "    return MSE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Function Implementation\n",
    "> The MultiGaussian() function is to implement the entire process of training and testing a multivariate\n",
    "Gaussian classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiGaussian(training_data: str, testing_data: str, Model: int):\n",
    "    if Model not in [1, 2, 3]:\n",
    "        raise Exception(\"'Model'should be set to 1, 2, or 3\")\n",
    "        \n",
    "    train_df = pd.read_csv(training_data, header=None)\n",
    "    C1 = train_df.groupby(8).get_group(1).drop(8, axis=1).to_numpy()\n",
    "    C2 = train_df.groupby(8).get_group(2).drop(8, axis=1).to_numpy()\n",
    "    P_C1 = C1.shape[0] / train_df.shape[0]\n",
    "    P_C2 = 1 - P_C1\n",
    "    m1 = C1.mean(axis=0)\n",
    "    m2 = C2.mean(axis=0)\n",
    "    \n",
    "    if Model == 1: model = IndependentCovar()\n",
    "    elif Model == 2: model = EqualCovar()\n",
    "    else: model = DiagonalCovar()\n",
    "    \n",
    "    S1, S2 = model.getCovar(C1, C2, P_C1, P_C2, m1, m2)\n",
    "    test = pd.read_csv(testing_data, header=None)\n",
    "    error = testError(model, test, P_C1, P_C2, m1, m2, S1, S2)\n",
    "    return pd.DataFrame({\n",
    "        \"P(C1)\": P_C1, \"P(C2)\": P_C2,\n",
    "        \"m1\": [m1], \"m2\": [m2],\n",
    "        \"S1\": [S1], \"S2\": [S2],\n",
    "        \"Test error\": error,\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation of parameters and error rates for all combinations of models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [(\"training_data1.txt\", \"test_data1.txt\"),\n",
    "              (\"training_data2.txt\", \"test_data2.txt\"),\n",
    "              (\"training_data3.txt\", \"test_data3.txt\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in data_files:\n",
    "    file_results = []\n",
    "    for Model in range(1, 4):\n",
    "        file_results.append(MultiGaussian(file[0], file[1], Model))\n",
    "    results.append(file_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing out parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1:\n",
      "  P(C1): 0.3\n",
      "  P(C2): 0.7\n",
      "  m1: [ 0.43  2.02  3.18 -2.43 -2.52  3.24 -5.52 -6.69]\n",
      "  m2: [ 4.58  6.49  6.43  1.69  2.29  8.36 -0.17 -1.8 ]\n",
      "  S1: [[ 1.86  0.23  0.75  1.    0.42  1.22  1.13 -1.19]\n",
      "       [ 0.23  3.54  0.3  -0.13  1.53  1.   -0.19  3.17]\n",
      "       [ 0.75  0.3   7.84  1.29 -0.41  1.72  0.34  0.22]\n",
      "       [ 1.   -0.13  1.29  4.09  0.92  0.72  1.03  1.91]\n",
      "       [ 0.42  1.53 -0.41  0.92  4.    0.97 -0.53  3.32]\n",
      "       [ 1.22  1.    1.72  0.72  0.97  3.93 -0.19  2.22]\n",
      "       [ 1.13 -0.19  0.34  1.03 -0.53 -0.19  4.08 -1.65]\n",
      "       [-1.19  3.17  0.22  1.91  3.32  2.22 -1.65 16.53]]\n",
      "  S2: [[ 3.42  2.07  2.57  2.61  1.77  1.83  2.68  2.93]\n",
      "       [ 2.07  5.78  2.18  2.72  3.16  2.89  2.76  5.85]\n",
      "       [ 2.57  2.18  8.71  3.38  2.83  2.23  2.75  5.18]\n",
      "       [ 2.61  2.72  3.38  8.17  3.58  2.66  2.02  8.4 ]\n",
      "       [ 1.77  3.16  2.83  3.58  5.57  2.91  3.25  4.82]\n",
      "       [ 1.83  2.89  2.23  2.66  2.91  3.73  2.23  4.48]\n",
      "       [ 2.68  2.76  2.75  2.02  3.25  2.23  8.21  4.31]\n",
      "       [ 2.93  5.85  5.18  8.4   4.82  4.48  4.31 19.85]]\n",
      "\n",
      "Model 2:\n",
      "  P(C1): 0.3\n",
      "  P(C2): 0.7\n",
      "  m1: [ 0.43  2.02  3.18 -2.43 -2.52  3.24 -5.52 -6.69]\n",
      "  m2: [ 4.58  6.49  6.43  1.69  2.29  8.36 -0.17 -1.8 ]\n",
      "  S1: [[ 2.96  1.52  2.02  2.13  1.37  1.65  2.22  1.7 ]\n",
      "       [ 1.52  5.11  1.62  1.86  2.67  2.32  1.87  5.05]\n",
      "       [ 2.02  1.62  8.45  2.75  1.85  2.08  2.03  3.7 ]\n",
      "       [ 2.13  1.86  2.75  6.94  2.78  2.08  1.72  6.45]\n",
      "       [ 1.37  2.67  1.85  2.78  5.1   2.33  2.12  4.37]\n",
      "       [ 1.65  2.32  2.08  2.08  2.33  3.79  1.51  3.8 ]\n",
      "       [ 2.22  1.87  2.03  1.72  2.12  1.51  6.97  2.52]\n",
      "       [ 1.7   5.05  3.7   6.45  4.37  3.8   2.52 18.85]]\n",
      "  S2: [[ 2.96  1.52  2.02  2.13  1.37  1.65  2.22  1.7 ]\n",
      "       [ 1.52  5.11  1.62  1.86  2.67  2.32  1.87  5.05]\n",
      "       [ 2.02  1.62  8.45  2.75  1.85  2.08  2.03  3.7 ]\n",
      "       [ 2.13  1.86  2.75  6.94  2.78  2.08  1.72  6.45]\n",
      "       [ 1.37  2.67  1.85  2.78  5.1   2.33  2.12  4.37]\n",
      "       [ 1.65  2.32  2.08  2.08  2.33  3.79  1.51  3.8 ]\n",
      "       [ 2.22  1.87  2.03  1.72  2.12  1.51  6.97  2.52]\n",
      "       [ 1.7   5.05  3.7   6.45  4.37  3.8   2.52 18.85]]\n",
      "\n",
      "Model 3:\n",
      "  P(C1): 0.3\n",
      "  P(C2): 0.7\n",
      "  m1: [ 0.43  2.02  3.18 -2.43 -2.52  3.24 -5.52 -6.69]\n",
      "  m2: [ 4.58  6.49  6.43  1.69  2.29  8.36 -0.17 -1.8 ]\n",
      "  S1: [[ 1.86  0.    0.    0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    3.54  0.    0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    7.84  0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    4.09  0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    4.    0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    3.93  0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    0.    4.08  0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    0.    0.   16.53]]\n",
      "  S2: [[ 3.42  0.    0.    0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    5.78  0.    0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    8.71  0.    0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    8.17  0.    0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    5.57  0.    0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    3.73  0.    0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    0.    8.21  0.  ]\n",
      "       [ 0.    0.    0.    0.    0.    0.    0.   19.85]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "for i in range(3):\n",
    "    print(f'\\nModel {i+1}:')\n",
    "    output = results[0][i]\n",
    "    print(\"  P(C1):\", output[\"P(C1)\"][0])\n",
    "    print(\"  P(C2):\", output[\"P(C2)\"][0])\n",
    "    print(\"  m1:\", output[\"m1\"][0])\n",
    "    print(\"  m2:\", output[\"m2\"][0])\n",
    "    print(\"  S1:\", str(output[\"S1\"][0]).replace('\\n', '\\n'))\n",
    "    print(\"  S2:\", str(output[\"S2\"][0]).replace('\\n', '\\n'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.2 Prediction accuracy of the learning method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error rate of each dataset with each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates:\n",
      "  For dataset 1:\n",
      "    M1: 0.22\n",
      "    M2: 0.17\n",
      "    M3: 0.18\n",
      "\n",
      "  For dataset 2:\n",
      "    M1: 0.23\n",
      "    M2: 0.55\n",
      "    M3: 0.52\n",
      "\n",
      "  For dataset 3:\n",
      "    M1: 0.11\n",
      "    M2: 0.45\n",
      "    M3: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Error rates:\", end='')\n",
    "for i in range(3):\n",
    "    print(f'\\n  For dataset {i+1}:')\n",
    "    for j in range(3):\n",
    "        print(f'    M{j+1}: {results[i][j][\"Test error\"][0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end we can conclude that : \n",
    "\n",
    "> For dataset 1 : Model 2 (Equal Covariances)\n",
    "\n",
    "> For dataset 2 : Model 1 (Independent Covariances)\n",
    "\n",
    "> For dataset 3 : Model 3 (Diagonal Covariances )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "End of Topic-B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
